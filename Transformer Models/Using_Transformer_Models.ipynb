{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "authorship_tag": "ABX9TyMGpl0Ed/c6omP+nQI287Id",
   "include_colab_link": true
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "view-in-github",
    "colab_type": "text"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/dp457/HF-Transformer/blob/main/Using_Transformer_Models.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0B3myn26CdUl"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "The transformers library created to solve the problem. Goal to provide single API where Transformer model can be loaded, trained and saved.\n",
    "\n",
    "The library’s main features are:\n",
    "\n",
    "**1. Ease of use:** Downloading, loading, and using a state-of-the-art NLP model for inference can be done in just two lines of code.\n",
    "\n",
    "**2. Flexibility:** At their core, all models are simple PyTorch nn.Module classes and can be handled like any other models in their respective machine learning (ML) frameworks.\n",
    "\n",
    "**3. Simplicity:** Hardly any abstractions are made across the library. The “All in one file” is a core concept: a model’s forward pass is entirely defined in a single file, so that the code itself is understandable and hackable."
   ],
   "metadata": {
    "id": "RHCH8BxdC764"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "!pip install datasets evaluate transformers[sentencepiece]"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qYznphm-GtEw",
    "outputId": "acde5ab9-65da-41bd-ec37-9d902da57e0c"
   },
   "execution_count": 2,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Requirement already satisfied: datasets in /usr/local/lib/python3.12/dist-packages (4.0.0)\n",
      "Collecting evaluate\n",
      "  Downloading evaluate-0.4.5-py3-none-any.whl.metadata (9.5 kB)\n",
      "Requirement already satisfied: transformers[sentencepiece] in /usr/local/lib/python3.12/dist-packages (4.56.0)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from datasets) (3.19.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from datasets) (2.0.2)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (18.1.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from datasets) (2.2.2)\n",
      "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.12/dist-packages (from datasets) (2.32.4)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.12/dist-packages (from datasets) (4.67.1)\n",
      "Requirement already satisfied: xxhash in /usr/local/lib/python3.12/dist-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2025.3.0)\n",
      "Requirement already satisfied: huggingface-hub>=0.24.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.34.4)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from datasets) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from datasets) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers[sentencepiece]) (2024.11.6)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers[sentencepiece]) (0.22.0)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers[sentencepiece]) (0.6.2)\n",
      "Requirement already satisfied: sentencepiece!=0.1.92,>=0.1.91 in /usr/local/lib/python3.12/dist-packages (from transformers[sentencepiece]) (0.2.1)\n",
      "Requirement already satisfied: protobuf in /usr/local/lib/python3.12/dist-packages (from transformers[sentencepiece]) (5.29.5)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.12.15)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.24.0->datasets) (4.15.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.24.0->datasets) (1.1.9)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets) (3.4.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets) (2025.8.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.4.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.7.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.6.4)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (0.3.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.20.1)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
      "Downloading evaluate-0.4.5-py3-none-any.whl (84 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.1/84.1 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: evaluate\n",
      "Successfully installed evaluate-0.4.5\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "classifier = pipeline(\"sentiment-analysis\")\n",
    "classifier(\n",
    "    [\n",
    "        \"I've been waiting for a HuggingFace course my whole life.\",\n",
    "        \"I hate this so much!\",\n",
    "    ]\n",
    ")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 364,
     "referenced_widgets": [
      "dfbe16e369874ad486d0543c09d52e12",
      "72e89a21daa848429a9275595bd86b37",
      "8a9e8100103d4ae3b12e413b46f21ac1",
      "2863384eb0604fcd8be0e3b5790e926c",
      "c489c964f5384feabd2c78c0e9eb8957",
      "c7f0b419040748329bbff6f823698691",
      "c54c8bf5bc1b4f75a298f2c1dd95f35e",
      "56d8415d68d6488c868d402cd15365c9",
      "f91c25aa907c452298296fd77f5fe1ce",
      "984ee55801624ae183029af03035a117",
      "96ed59abf3dc432d89ed97642dc8465e",
      "58ad57727f944fb38b0c47bb167324d9",
      "32ac4aa5805940909f900ea1646fddcc",
      "211ada38291e4ee1bc81ca881cf3f3af",
      "f94f703398aa4c59b3e00b3781a4be33",
      "c97802831bcb4592a931fcd4b096bedf",
      "1cf9d2e086b842f08dc64a527d32296c",
      "3973e4a554f34a42979fc9a21b920f79",
      "e599120320034b798c3cae1368bc85ee",
      "f2f074f5c5b8421eb6879dc92dbf7bb0",
      "5318ec7e78c945668035d068c604cb9b",
      "4429b29187f04592b7e766836754e4a2",
      "51a5442f26464dc18cc059d8f9fc1a4f",
      "2f2a84259c4e4356b166fb1336fe1d99",
      "247c6f819ee54658b07b12a3e668ce4a",
      "5a26dcb456724c3baf9159e989f5e53b",
      "88ae09333bf94ce1b93084d6615c22d4",
      "15e7914368864cde8e55a8da0f077501",
      "7f3be2b7d4584316a99a474007a2eb88",
      "5f16face939a4218acd73d43766290eb",
      "682594fb41b644b594db89f58e064a7d",
      "0053dbb09e154a6d8224226473a3a9ae",
      "4b7b19b43a2c4bac942243dbc3bfb84e",
      "565f7491b53d406a98e971c18779940a",
      "04d1fe57edfb438eac26286054e5bd4b",
      "3a330657dc1d41d9b90c793d75400d86",
      "b4a6694822134d3488c8502f832dd3e2",
      "ecb78a4ed193400b89efd17a9f534cec",
      "30b0f62fb83e40009e3f9d3d6f0c575a",
      "98c4053a6fbd48bd86d698fdde66cb52",
      "0fe39626306c4a30976477816f4c7ba9",
      "0afce948e10a40579497a3c3f65aa78c",
      "8afa7682dd264149ab6e5010034e7a0f",
      "5a57f67c7c1f49d49e42388cc0c4e4f1"
     ]
    },
    "id": "nSKMAoiJG7y_",
    "outputId": "09654d15-f8aa-4e54-f370-69e80726902e"
   },
   "execution_count": 3,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "No model was supplied, defaulted to distilbert/distilbert-base-uncased-finetuned-sst-2-english and revision 714eb0f (https://huggingface.co/distilbert/distilbert-base-uncased-finetuned-sst-2-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
      "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
      "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
      "You will be able to reuse this secret in all of your notebooks.\n",
      "Please note that authentication is recommended but still optional to access public models or datasets.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "config.json:   0%|          | 0.00/629 [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "dfbe16e369874ad486d0543c09d52e12"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/268M [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "58ad57727f944fb38b0c47bb167324d9"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "51a5442f26464dc18cc059d8f9fc1a4f"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "vocab.txt: 0.00B [00:00, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "565f7491b53d406a98e971c18779940a"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[{'label': 'POSITIVE', 'score': 0.9598049521446228},\n",
       " {'label': 'NEGATIVE', 'score': 0.9994558691978455}]"
      ]
     },
     "metadata": {},
     "execution_count": 3
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Preprocessing with a tokenizer**\n",
    "\n",
    "Like other neural networks, Transformer models can’t process raw text directly, so the first step of our pipeline is to convert the text inputs into numbers that the model can make sense of. To do this a tokenizer is used , which will be responsible for:\n",
    "\n",
    "1. Splitting the input into words, subwords, or symbols (like punctuation) that are called tokens\n",
    "2. Mapping each token to an integer\n",
    "3. Adding additional inputs that may be useful to the model\n",
    "\n",
    "All this preprocessing needs to be done in exactly the same way as when the model was pretrained, so the information has to be downloaded accordingly.\n",
    "\n",
    "Here `AutoTokenizer` class is used and its `from_pretrained()` method. Using the checkpoint name of the model, it will automatically fetch the data associated with the mtokenizer of the model and its cached.\n",
    "\n",
    "Since the default checkpoint of the sentiment-analysis pipeline is distilbert-base-uncased-finetuned-sst-2-english, the following code is run:"
   ],
   "metadata": {
    "id": "6ZYaAMwkHDcv"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 113,
     "referenced_widgets": [
      "6d26b6ded2d849418a3ddd0dc7f5fbf4",
      "c85d4c69de7c4a27919706c7a0228c83",
      "715d4bf23f32458f8b97bf0c46310eef",
      "6325cb23df144532b3a20515e8b421b8",
      "082228bee3d34fefa68984aeecce6e86",
      "8ff53bcc130f4f34a738841d40c62518",
      "29800e22db6b49f0b13d0c9a2bdfa541",
      "25468a2443594d5c994eb6448c8335c8",
      "f8d1235b40414cd3aaec2b3b44dd81f2",
      "2c3ca2ef87504bc9988b0bbc8b2a8113",
      "28655ca8ebc644a9b4203f47134c6323",
      "08570387c3da4b6b914b84e5dfb4af48",
      "400106e1e02b416c873125e5c2e73281",
      "0798121b22f148bb9ff96b6841cd15a0",
      "9900ad89bc474c89ba94cb973409f449",
      "149fe5b5fac5457db82276c610f3e225",
      "e7fa954d7b62445eafb478d5ed189942",
      "8eae1ea1a4e2432d8bbd943533bd38ac",
      "796878e4b25a4f6fb2337d4d1a3ba651",
      "9656d7de639748eab928b27e5bc42011",
      "73d6e558cba14fb0a546a8963a764ecb",
      "d42c05ee2484464a8f8010230d34e8ce",
      "221d6acbac9d453b811b1d4c72557478",
      "60ee453e7a7b4893b73aee25bd4dcab9",
      "0001ac2487fd473c8c82bef73f06bbd7",
      "d9350a6f23694338b2d8f939deb72ea9",
      "fc4749794d614665af906711d15f3d50",
      "44fd26e0ac4c418ba537e17bfaaae52e",
      "9d692293a2e8422ebe1c37935599292a",
      "0b3872f9c5ee4beba0b664dc42f105e7",
      "2a522fce03da416d90ae39b9d3d7b605",
      "fa361fb69e664e6a9222515a190b8c94",
      "9f4a5a67015c4e6697e03eaba2645425"
     ]
    },
    "id": "F2rwvFHFHoNi",
    "outputId": "93d9b67a-da41-4817-c4a2-fb187e296f62"
   },
   "execution_count": 4,
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "6d26b6ded2d849418a3ddd0dc7f5fbf4"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "config.json:   0%|          | 0.00/629 [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "08570387c3da4b6b914b84e5dfb4af48"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "221d6acbac9d453b811b1d4c72557478"
      }
     },
     "metadata": {}
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Once the tokenizer is obtained, the sentences can be passed to it and a dictionary is obtained that’s ready to feed to our model. Hence the list of input IDs have to be convered into tensors.\n",
    "\n",
    "Transformer models only accept tensors as input. If this is your first time hearing about tensors, you can think of them as NumPy arrays instead. A NumPy array can be a scalar (0D), a vector (1D), a matrix (2D), or have more dimensions. It’s effectively a tensor; other ML frameworks’ tensors behave similarly, and are usually as simple to instantiate as NumPy arrays."
   ],
   "metadata": {
    "id": "j4DHwI1fHuNi"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "raw_inputs = [\n",
    "    \"I've been waiting for a HuggingFace course my whole life.\",\n",
    "    \"I hate this so much!\",\n",
    "]\n",
    "inputs = tokenizer(raw_inputs, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "print(inputs)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BnNIXoQcKYJv",
    "outputId": "8febc2e4-e90a-42a7-d67a-5dae7d63756a"
   },
   "execution_count": 5,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "{'input_ids': tensor([[  101,  1045,  1005,  2310,  2042,  3403,  2005,  1037, 17662, 12172,\n",
      "          2607,  2026,  2878,  2166,  1012,   102],\n",
      "        [  101,  1045,  5223,  2023,  2061,  2172,   999,   102,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0]])}\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "The output itself is a dictionary containing two keys, `input_ids` and `attention_mask`. `input_ids` contains two rows of integers (one for each sentence) that are the unique identifiers of the tokens in each sentence.\n",
    "\n",
    "In this code snippet, same checkpoint has been downloaded which was used in the pipeline before (it should actually have been cached already) and instantiated a model with it.\n",
    "\n",
    "This architecture contains only the base **Transformer** module: given some inputs, it outputs are called hidden states also known as features.\n",
    "\n",
    "For each model input, a high-dimensional vector is retrieved which represents the contextual understanding of that input by the Transformer model.\n",
    "\n",
    "While these hidden states can be useful on their own, they’re usually inputs to another part of the model, known as the **head**.\n",
    "\n",
    "***A high-dimensional vectors??***\n",
    "\n",
    "The vector output by the Transformer module is usually large. It generally has three dimensions:\n",
    "\n",
    "* Batch size: The number of sequences processed at a time (2 in our example).\n",
    "* Sequence length: The length of the numerical representation of the sequence (16 in our example).\n",
    "* Hidden size: The vector dimension of each model input.\n"
   ],
   "metadata": {
    "id": "5b6NxVxlKd-n"
   }
  }
 ]
}